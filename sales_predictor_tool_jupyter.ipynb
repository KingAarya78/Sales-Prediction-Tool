{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sales Prediction Using Python\n",
    "\n",
    "This notebook demonstrates how to build a sales prediction model based on advertising spend across different channels (TV, radio, newspaper). We'll go through the entire data science pipeline including exploratory data analysis, data cleaning, feature engineering, model selection, and extracting business insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies\n",
    "\n",
    "First, let's import all the necessary libraries for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os  # For file operations\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Functions\n",
    "\n",
    "Let's define some utility functions to help with our workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "def ensure_output_directory(directory='output'):\n",
    "    \"\"\"\n",
    "    Creates output directory if it doesn't exist\n",
    "    \n",
    "    Parameters:\n",
    "    - directory: Directory path for saving outputs\n",
    "    \n",
    "    Returns:\n",
    "    - Directory path\n",
    "    \"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        print(f\"Created output directory: {directory}\")\n",
    "    else:\n",
    "        print(f\"Using existing output directory: {directory}\")\n",
    "    return directory\n",
    "\n",
    "# Function to safely save figures without overwriting\n",
    "def save_figure(fig_path):\n",
    "    \"\"\"\n",
    "    Saves a figure only if it doesn't already exist\n",
    "    \n",
    "    Parameters:\n",
    "    - fig_path: Full path for saving the figure\n",
    "    \"\"\"\n",
    "    if not os.path.exists(fig_path):\n",
    "        plt.savefig(fig_path)\n",
    "        print(f\"Saved: {fig_path}\")\n",
    "    else:\n",
    "        print(f\"File already exists (not overwritten): {fig_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading\n",
    "\n",
    "Let's load our dataset. If the file isn't found, we'll create sample data for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = ensure_output_directory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv('advertising.csv')\n",
    "    \n",
    "    # Clean column names\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "    print(\"Dataset loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: advertising.csv file not found.\")\n",
    "    print(\"Creating sample data for demonstration...\")\n",
    "    \n",
    "    # Create sample data\n",
    "    np.random.seed(42)\n",
    "    n = 200\n",
    "    tv = np.random.uniform(0, 300, n)\n",
    "    radio = np.random.uniform(0, 50, n)\n",
    "    newspaper = np.random.uniform(0, 100, n)\n",
    "    \n",
    "    # Create sales with realistic relationships\n",
    "    sales = 4.5 + 0.05*tv + 0.25*radio + 0.1*newspaper + 0.0003*tv*radio + np.random.normal(0, 3, n)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'tv': tv,\n",
    "        'radio': radio,\n",
    "        'newspaper': newspaper,\n",
    "        'sales': sales\n",
    "    })\n",
    "    \n",
    "    # Add some missing values for demonstration\n",
    "    for col in df.columns:\n",
    "        mask = np.random.random(len(df)) < 0.05\n",
    "        df.loc[mask, col] = np.nan\n",
    "\n",
    "# Display the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Let's explore our dataset to understand its characteristics and relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_data(df, output_dir):\n",
    "    \"\"\"\n",
    "    Perform exploratory data analysis on the dataset\n",
    "    \"\"\"\n",
    "    print(\"Dataset Information:\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    \n",
    "    print(\"\\nStatistical Summary:\")\n",
    "    display(df.describe())\n",
    "    \n",
    "    print(\"\\nCorrelation Analysis:\")\n",
    "    correlation = df.corr()\n",
    "    print(correlation['sales'].sort_values(ascending=False))\n",
    "    \n",
    "    # Correlation heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(correlation, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "    plt.title('Correlation Heatmap')\n",
    "    plt.tight_layout()\n",
    "    save_figure(os.path.join(output_dir, 'correlation_heatmap.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    # Create a single figure for pairplot\n",
    "    sns.pairplot(df, x_vars=['tv', 'radio', 'newspaper'], y_vars='sales', height=4)\n",
    "    plt.suptitle('Relationship between Advertising Channels and Sales', y=1.02)\n",
    "    save_figure(os.path.join(output_dir, 'channel_relationships.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    # Distribution of target variable\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(df['sales'], kde=True)\n",
    "    plt.title('Distribution of Sales')\n",
    "    plt.xlabel('Sales')\n",
    "    plt.ylabel('Frequency')\n",
    "    save_figure(os.path.join(output_dir, 'sales_distribution.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Run EDA\n",
    "df = explore_data(df, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Cleaning\n",
    "\n",
    "Now let's clean our data by handling missing values and outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    Handle missing values and outliers in the dataset\n",
    "    \"\"\"\n",
    "    print(\"\\nCleaning Data:\")\n",
    "    print(f\"Missing values before cleaning: {df.isnull().sum().sum()}\")\n",
    "    \n",
    "    # Handle missing values\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype.kind in 'ifc':  # if column is integer, float or complex\n",
    "            df[col].fillna(df[col].median(), inplace=True)\n",
    "        else:\n",
    "            df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "    \n",
    "    print(f\"Missing values after cleaning: {df.isnull().sum().sum()}\")\n",
    "    \n",
    "    # Remove outliers using IQR method\n",
    "    print(\"\\nOutlier Treatment:\")\n",
    "    rows_before = len(df)\n",
    "    \n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()\n",
    "        print(f\"Column '{col}': {outliers} outliers detected\")\n",
    "        \n",
    "        df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
    "    \n",
    "    rows_after = len(df)\n",
    "    print(f\"Rows removed due to outliers: {rows_before - rows_after}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Clean the data\n",
    "df = clean_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering\n",
    "\n",
    "Let's create new features to potentially improve our model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "    \"\"\"\n",
    "    Create new features to improve model performance\n",
    "    \"\"\"\n",
    "    print(\"\\nFeature Engineering:\")\n",
    "    \n",
    "    # Create interaction terms between advertising channels\n",
    "    df['tv_radio'] = df['tv'] * df['radio']\n",
    "    df['tv_newspaper'] = df['tv'] * df['newspaper']\n",
    "    df['radio_newspaper'] = df['radio'] * df['newspaper']\n",
    "    \n",
    "    # Create squared terms to capture non-linear relationships\n",
    "    df['tv_squared'] = df['tv'] ** 2\n",
    "    df['radio_squared'] = df['radio'] ** 2\n",
    "    df['newspaper_squared'] = df['newspaper'] ** 2\n",
    "    \n",
    "    # Create ratio features\n",
    "    df['tv_ratio'] = df['tv'] / (df['radio'] + 1)  # Adding 1 to avoid division by zero\n",
    "    df['newspaper_ratio'] = df['newspaper'] / (df['tv'] + 1)\n",
    "    \n",
    "    print(f\"Original features: {['tv', 'radio', 'newspaper']}\")\n",
    "    print(f\"New features added: {[col for col in df.columns if col not in ['tv', 'radio', 'newspaper', 'sales']]}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Add new features\n",
    "df = feature_engineering(df)\n",
    "\n",
    "# Display the first few rows with new features\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Selection\n",
    "\n",
    "Let's identify the most important features for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "X = df.drop('sales', axis=1)\n",
    "y = df['sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(X, y, output_dir):\n",
    "    \"\"\"\n",
    "    Identify the most important features for the model\n",
    "    \"\"\"\n",
    "    print(\"\\nFeature Importance Analysis:\")\n",
    "    \n",
    "    # Use Random Forest for feature importance\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf.fit(X, y)\n",
    "    \n",
    "    # Get feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': rf.feature_importances_\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    display(feature_importance)\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
    "    plt.title('Feature Importance for Sales Prediction')\n",
    "    plt.tight_layout()\n",
    "    save_figure(os.path.join(output_dir, 'feature_importance.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    return feature_importance\n",
    "\n",
    "# Identify important features\n",
    "feature_importance = feature_selection(X, y, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Splitting and Scaling\n",
    "\n",
    "Now let's split our data into training and testing sets and scale our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Data Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to dataframes to keep column names\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "print(f\"Training set shape: {X_train_scaled.shape}\")\n",
    "print(f\"Test set shape: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Selection and Evaluation\n",
    "\n",
    "Let's build and evaluate multiple models to find the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_selection(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Select the best model and tune hyperparameters\n",
    "    \"\"\"\n",
    "    print(\"\\nModel Selection and Evaluation:\")\n",
    "    \n",
    "    # Define base models\n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Random Forest': RandomForestRegressor(random_state=42),\n",
    "        'XGBoost': XGBRegressor(random_state=42, eval_metric='rmse')\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        # Cross-validation\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "        cv_rmse = np.sqrt(-cv_scores.mean())\n",
    "        \n",
    "        # Train model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Test predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        test_r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        results[name] = {\n",
    "            'cv_rmse': cv_rmse,\n",
    "            'test_rmse': test_rmse,\n",
    "            'test_r2': test_r2,\n",
    "            'model': model\n",
    "        }\n",
    "        \n",
    "        print(f\"{name} - CV RMSE: {cv_rmse:.4f}, Test RMSE: {test_rmse:.4f}, R²: {test_r2:.4f}\")\n",
    "    \n",
    "    # Find best model\n",
    "    best_model_name = min(results, key=lambda x: results[x]['test_rmse'])\n",
    "    best_model = results[best_model_name]['model']\n",
    "    print(f\"\\nBest Model: {best_model_name}\")\n",
    "    \n",
    "    # Hyperparameter tuning for best model\n",
    "    if best_model_name == 'Linear Regression':\n",
    "        # Linear Regression doesn't need much tuning\n",
    "        final_model = best_model\n",
    "    \n",
    "    elif best_model_name == 'Random Forest':\n",
    "        print(\"\\nTuning Random Forest Hyperparameters...\")\n",
    "        param_grid = {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "        grid_search = GridSearchCV(RandomForestRegressor(random_state=42), \n",
    "                                  param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        final_model = grid_search.best_estimator_\n",
    "        print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    \n",
    "    elif best_model_name == 'XGBoost':\n",
    "        print(\"\\nTuning XGBoost Hyperparameters...\")\n",
    "        param_grid = {\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'n_estimators': [100, 200, 300]\n",
    "        }\n",
    "        grid_search = GridSearchCV(XGBRegressor(random_state=42, eval_metric='rmse'), \n",
    "                                  param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        final_model = grid_search.best_estimator_\n",
    "        print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Final evaluation\n",
    "    final_model.fit(X_train, y_train)\n",
    "    y_pred = final_model.predict(X_test)\n",
    "    final_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    final_r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\nFinal Model Performance - RMSE: {final_rmse:.4f}, R²: {final_r2:.4f}\")\n",
    "    \n",
    "    return final_model, y_pred\n",
    "\n",
    "# Select the best model\n",
    "best_model, y_pred = model_selection(X_train_scaled, y_train, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualize Predictions\n",
    "\n",
    "Let's visualize how well our model predicts the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(y_test, y_pred, X_test, output_dir):\n",
    "    \"\"\"\n",
    "    Visualize model predictions against actual values\n",
    "    \"\"\"\n",
    "    # Actual vs Predicted\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.7)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.xlabel('Actual Sales')\n",
    "    plt.ylabel('Predicted Sales')\n",
    "    plt.title('Actual vs Predicted Sales')\n",
    "    plt.tight_layout()\n",
    "    save_figure(os.path.join(output_dir, 'actual_vs_predicted.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    # Residual plot\n",
    "    residuals = y_test - y_pred\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_pred, residuals, alpha=0.7)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.xlabel('Predicted Sales')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title('Residual Plot')\n",
    "    plt.tight_layout()\n",
    "    save_figure(os.path.join(output_dir, 'residual_plot.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    # Distribution of residuals\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(residuals, kde=True)\n",
    "    plt.xlabel('Residuals')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Residuals')\n",
    "    plt.tight_layout()\n",
    "    save_figure(os.path.join(output_dir, 'residual_distribution.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    # If we have feature data, show predictions by TV advertising spend\n",
    "    if X_test is not None and 'tv' in X_test.columns:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(X_test['tv'], y_test, alpha=0.7, label='Actual')\n",
    "        plt.scatter(X_test['tv'], y_pred, alpha=0.7, label='Predicted')\n",
    "        plt.xlabel('TV Advertising Spend')\n",
    "        plt.ylabel('Sales')\n",
    "        plt.title('Sales vs TV Advertising')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        save_figure(os.path.join(output_dir, 'tv_predictions.png'))\n",
    "        plt.show()\n",
    "\n",
    "# Visualize predictions\n",
    "plot_predictions(y_test, y_pred, X_test, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Extract Business Insights\n",
    "\n",
    "Finally, let's extract business insights and recommendations from our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def business_insights(model, X, feature_importance, output_dir):\n",
    "    \"\"\"\n",
    "    Extract business insights from the model\n",
    "    \"\"\"\n",
    "    print(\"\\nBusiness Insights and Recommendations:\")\n",
    "    \n",
    "    # Most influential channels\n",
    "    top_features = feature_importance.head(3)['Feature'].values\n",
    "    print(f\"Top influencing factors for sales: {', '.join(top_features)}\")\n",
    "    \n",
    "    # Coefficient analysis (for linear regression)\n",
    "    if hasattr(model, 'coef_'):\n",
    "        coef_df = pd.DataFrame({\n",
    "            'Feature': X.columns,\n",
    "            'Coefficient': model.coef_\n",
    "        }).sort_values(by='Coefficient', ascending=False)\n",
    "        \n",
    "        print(\"\\nImpact of each advertising channel (Linear Regression Coefficients):\")\n",
    "        display(coef_df)\n",
    "        \n",
    "        # Plot coefficients\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.barplot(x='Coefficient', y='Feature', data=coef_df)\n",
    "        plt.title('Impact of Advertising Channels on Sales')\n",
    "        plt.tight_layout()\n",
    "        save_figure(os.path.join(output_dir, 'coefficient_impact.png'))\n",
    "        plt.show()\n",
    "    \n",
    "    # Recommendations\n",
    "    print(\"\\nRecommendations for Advertising Strategy:\")\n",
    "    print(\"1. Focus on the most influential advertising channels\")\n",
    "    print